{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 89453,
     "status": "ok",
     "timestamp": 1758736820551,
     "user": {
      "displayName": "Liu Zhengyang",
      "userId": "06130671106158642277"
     },
     "user_tz": -480
    },
    "id": "QcOt1SkNyqI_",
    "outputId": "7631d1de-da65-47e3-b62b-99b53dbf2ae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Columns: ['customerID', 'ISIN', 'transactionID', 'transactionType', 'timestamp', 'totalValue', 'units', 'channel', 'marketID']\n",
      "Shape: (388048, 9)\n",
      "Users: 14175, Items: 303\n",
      "ALS iter 5/15\n",
      "ALS iter 10/15\n",
      "ALS iter 15/15\n",
      "Users=14175, Items=303, Train events=196630, Test users=14175\n",
      "ALS (NumPy)  | HR@10: 0.1367  Recall@10: 0.1367  NDCG@10: 0.0637\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1) Mount Google Drive\n",
    "# ============================================================\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# ============================================================\n",
    "# 2) Setup paths & constants\n",
    "# ============================================================\n",
    "CSV_PATH = \"../data/raw/transactions.csv\"\n",
    "\n",
    "USER_COL  = \"customerID\"\n",
    "ITEM_COL  = \"ISIN\"\n",
    "TS_COL    = \"timestamp\"\n",
    "TYPE_COL  = \"transactionType\"\n",
    "VALUE_COL = \"totalValue\"\n",
    "\n",
    "USE_VALUE_WEIGHT = True\n",
    "INCLUDE_SELL     = False\n",
    "K = 10\n",
    "MIN_USER_EVENTS = 2\n",
    "\n",
    "# ============================================================\n",
    "# 3) Load + clean data\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df[TS_COL] = pd.to_datetime(df[TS_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[USER_COL, ITEM_COL, TS_COL, TYPE_COL])\n",
    "\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "print(\"Shape:\", df.shape)\n",
    "\n",
    "# assign weights\n",
    "def w(row):\n",
    "    if str(row[TYPE_COL]).lower() == \"buy\":\n",
    "        if USE_VALUE_WEIGHT and VALUE_COL in df.columns:\n",
    "            v = float(row.get(VALUE_COL, 1.0) or 1.0)\n",
    "            return np.log1p(max(v, 0.0))  # log-scaled\n",
    "        return 1.0\n",
    "    return 0.0 if not INCLUDE_SELL else 0.1\n",
    "\n",
    "df[\"weight\"] = df.apply(w, axis=1)\n",
    "df = df[df[\"weight\"] > 0]\n",
    "\n",
    "# filter out users with too few events\n",
    "ucount = df.groupby(USER_COL)[ITEM_COL].nunique()\n",
    "df = df[df[USER_COL].isin(ucount[ucount >= MIN_USER_EVENTS].index)].copy()\n",
    "\n",
    "# encode to integer IDs\n",
    "user2id = {u:i for i,u in enumerate(df[USER_COL].astype(str).unique())}\n",
    "item2id = {i:j for j,i in enumerate(df[ITEM_COL].astype(str).unique())}\n",
    "df[\"u\"] = df[USER_COL].astype(str).map(user2id)\n",
    "df[\"i\"] = df[ITEM_COL].astype(str).map(item2id)\n",
    "\n",
    "n_users, n_items = len(user2id), len(item2id)\n",
    "print(f\"Users: {n_users}, Items: {n_items}\")\n",
    "\n",
    "# leave-last-one-out split\n",
    "df = df.sort_values([USER_COL, TS_COL])\n",
    "last  = df.groupby(\"u\").tail(1)\n",
    "train = pd.concat([df, last]).drop_duplicates(keep=False)\n",
    "\n",
    "# known items per user\n",
    "from collections import defaultdict\n",
    "known_items = defaultdict(set)\n",
    "for u, g in train.groupby(\"u\"):\n",
    "    known_items[u] = set(g[\"i\"].tolist())\n",
    "\n",
    "# ground-truth dict {user: [last_item]}\n",
    "test_truth = last.groupby(\"u\")[\"i\"].apply(list).to_dict()\n",
    "\n",
    "# ============================================================\n",
    "# 4) Fallback ALS (explicit MF via NumPy)\n",
    "# ============================================================\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# binary user-item matrix for training\n",
    "ui = csr_matrix((np.ones(len(train)), (train[\"u\"], train[\"i\"])), shape=(n_users, n_items))\n",
    "\n",
    "# ===== Correct explicit-ALS updates (per-user / per-item) =====\n",
    "f   = 64\n",
    "lam = 0.1\n",
    "iters = 15\n",
    "rng = np.random.default_rng(42)\n",
    "U = rng.normal(scale=0.01, size=(n_users, f)).astype(np.float32)\n",
    "V = rng.normal(scale=0.01, size=(n_items, f)).astype(np.float32)\n",
    "I_f = np.eye(f, dtype=np.float32)\n",
    "\n",
    "# ui: CSR (n_users x n_items) with 1 for observed in TRAIN (as you already built)\n",
    "for it in range(iters):\n",
    "    # --- Update U: solve (V_I^T V_I + λI) U_u = V_I^T r_u ---\n",
    "    for u in range(n_users):\n",
    "        idx = ui[u].indices\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "        V_I = V[idx]                      # (#items_u, f)\n",
    "        A = V_I.T @ V_I + lam * I_f       # (f, f)\n",
    "        b = V_I.sum(axis=0)               # r_u is ones → V_I^T * 1 = sum of rows\n",
    "        U[u] = np.linalg.solve(A, b)\n",
    "\n",
    "    # --- Update V: solve (U_U^T U_U + λI) V_i = U_U^T r_i ---\n",
    "    for i in range(n_items):\n",
    "        idx = ui[:, i].indices\n",
    "        if idx.size == 0:\n",
    "            continue\n",
    "        U_U = U[idx]                      # (#users_i, f)\n",
    "        A = U_U.T @ U_U + lam * I_f\n",
    "        b = U_U.sum(axis=0)               # r_i is ones\n",
    "        V[i] = np.linalg.solve(A, b)\n",
    "\n",
    "    if (it+1) % 5 == 0:\n",
    "        print(f\"ALS iter {it+1}/{iters}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# 5) Recommender & Evaluation\n",
    "# ============================================================\n",
    "UVT = U @ V.T  # predicted scores\n",
    "\n",
    "def rec_als_np(u, k=K):\n",
    "    scores = UVT[u].copy()\n",
    "    for i in known_items.get(u, []):\n",
    "        scores[i] = -np.inf\n",
    "    k_eff = min(k, max(1, scores.size - 1))\n",
    "    top = np.argpartition(-scores, k_eff-1)[:k_eff]\n",
    "    return list(top[np.argsort(-scores[top])][:k])\n",
    "\n",
    "def hit_at_k(recs, truth):   return 1.0 if any(t in recs for t in truth) else 0.0\n",
    "def recall_at_k(recs, truth):return len(set(recs) & set(truth)) / len(truth)\n",
    "def ndcg_at_k(recs, truth):\n",
    "    dcg = 0.0\n",
    "    for r, i in enumerate(recs, start=1):\n",
    "        if i in truth: dcg += 1.0 / np.log2(r + 1)\n",
    "    return dcg\n",
    "\n",
    "def evaluate(rec_fn, name, k=K):\n",
    "    HR, REC, NDCG = [], [], []\n",
    "    for u, truth in test_truth.items():\n",
    "        recs = rec_fn(u, k)\n",
    "        HR.append(hit_at_k(recs, truth))\n",
    "        REC.append(recall_at_k(recs, truth))\n",
    "        NDCG.append(ndcg_at_k(recs, truth))\n",
    "    print(f\"{name:12} | HR@{k}: {np.mean(HR):.4f}  Recall@{k}: {np.mean(REC):.4f}  NDCG@{k}: {np.mean(NDCG):.4f}\")\n",
    "\n",
    "print(f\"Users={n_users}, Items={n_items}, Train events={train.shape[0]}, Test users={len(test_truth)}\")\n",
    "evaluate(rec_als_np, \"ALS (NumPy)\", K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_JaZuRqzSgK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMA0Ge96h+upqS7yFXtpwQL",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
