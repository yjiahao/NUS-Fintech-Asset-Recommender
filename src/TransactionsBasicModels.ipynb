{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tx96RIjEuGc9"
   },
   "outputs": [],
   "source": [
    "# !pip -q install pandas numpy scipy scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjMYQqFohgoR"
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9t8VtA8_wSAB"
   },
   "outputs": [],
   "source": [
    "CSV_PATH = \"../data/raw/transactions.csv\"  # change if needed\n",
    "USER_COL = \"customerID\"\n",
    "ITEM_COL = \"ISIN\"\n",
    "TS_COL   = \"timestamp\"\n",
    "TYPE_COL = \"transactionType\"\n",
    "VALUE_COL = \"totalValue\"   # optional weight\n",
    "USE_VALUE_WEIGHT = True    # False => every Buy counts as 1\n",
    "INCLUDE_SELL = False       # True => tiny positive weight for sells\n",
    "K = 10\n",
    "MIN_USER_EVENTS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47605,
     "status": "ok",
     "timestamp": 1758730241289,
     "user": {
      "displayName": "Liu Zhengyang",
      "userId": "06130671106158642277"
     },
     "user_tz": -480
    },
    "id": "xei5RKhTwpLU",
    "outputId": "98330a66-63bb-4816-d7ff-b3d1ef8efb5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sW_itU2Ryjgz"
   },
   "outputs": [],
   "source": [
    "# ==== LOAD & CLEAN ====\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "req = {USER_COL, ITEM_COL, TS_COL, TYPE_COL}\n",
    "missing = req - set(df.columns)\n",
    "if missing: raise ValueError(f\"Missing columns: {missing}\")\n",
    "\n",
    "#turn timestamp string into real datetime\n",
    "df[TS_COL] = pd.to_datetime(df[TS_COL], errors=\"coerce\")\n",
    "df = df.dropna(subset=[USER_COL, ITEM_COL, TS_COL, TYPE_COL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "egiH1VFlxJ6q"
   },
   "outputs": [],
   "source": [
    "#turn rows into strengths of interaction\n",
    "def event_weight(row):\n",
    "    t = str(row[TYPE_COL]).strip().lower()\n",
    "    if t == \"buy\":\n",
    "        if USE_VALUE_WEIGHT and VALUE_COL in df.columns:\n",
    "            # damp large values so whales don't dominate: log(1 + value)\n",
    "            v = float(row.get(VALUE_COL, 1.0) or 1.0)\n",
    "            return np.log1p(max(v, 0.0))\n",
    "        return 1.0\n",
    "    if t == \"sell\":\n",
    "        # For baselines, we usually ignore sells; optionally give a tiny signal\n",
    "        return 0.1 if INCLUDE_SELL else 0.0\n",
    "    # Unknown/other transaction types → no signal\n",
    "    return 0.0\n",
    "\n",
    "df[\"weight\"] = df.apply(event_weight, axis=1)\n",
    "df = df[df[\"weight\"] > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oPgPS_B0uPL"
   },
   "outputs": [],
   "source": [
    "#Leave the last one out and see if can predict most recent value\n",
    "MIN_USER_EVENTS = 2\n",
    "ucount = df.groupby(USER_COL)[ITEM_COL].nunique()\n",
    "df = df[df[USER_COL].isin(ucount[ucount >= MIN_USER_EVENTS].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xpI1qf9T2PsO"
   },
   "outputs": [],
   "source": [
    "#encode string to integer\n",
    "uid2idx = {u:i for i,u in enumerate(df[USER_COL].astype(str).unique())}\n",
    "iid2idx = {i:j for j,i in enumerate(df[ITEM_COL].astype(str).unique())}\n",
    "idx2iid = {v:k for k,v in iid2idx.items()}  # decode later if needed\n",
    "\n",
    "df[\"u\"] = df[USER_COL].astype(str).map(uid2idx)\n",
    "df[\"i\"] = df[ITEM_COL].astype(str).map(iid2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33502,
     "status": "ok",
     "timestamp": 1758730839236,
     "user": {
      "displayName": "Liu Zhengyang",
      "userId": "06130671106158642277"
     },
     "user_tz": -480
    },
    "id": "IC-cKYmU0ozw",
    "outputId": "652334a5-d4fb-4c77-f1cc-5b05ce635146"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Users: 14175, Items: 303, Train events: 64898, Test users: 14175\n",
      "Popularity   | HR@10: 0.1598  Recall@10: 0.1598  NDCG@10: 0.0874\n",
      "Item-KNN     | HR@10: 0.4154  Recall@10: 0.4154  NDCG@10: 0.2692\n",
      "TruncSVD     | HR@10: 0.2357  Recall@10: 0.2357  NDCG@10: 0.1528\n"
     ]
    }
   ],
   "source": [
    "# ==== TIME-AWARE SPLIT: leave-last-one-out. hide the latest event ====\n",
    "df = df.sort_values([USER_COL, TS_COL])\n",
    "last = df.groupby(\"u\").tail(1)          # CHANGE THIS TO CHANGE RECALL@10\n",
    "train = pd.concat([df, last]).drop_duplicates(keep=False)\n",
    "\n",
    "\n",
    "#sparse interaction matrix\n",
    "n_users = len(uid2idx); n_items = len(iid2idx)\n",
    "\n",
    "def to_csr(frame):\n",
    "    return csr_matrix((frame[\"weight\"], (frame[\"u\"], frame[\"i\"])),\n",
    "                      shape=(n_users, n_items))\n",
    "\n",
    "X = to_csr(train)\n",
    "\n",
    "# ground-truth for eval (list of true test items)\n",
    "test_truth = last.groupby(\"u\")[\"i\"].apply(list).to_dict()\n",
    "\n",
    "\n",
    "\n",
    "# ==== MODELS ====\n",
    "# 1) Popularity\n",
    "item_pop = np.asarray(X.sum(axis=0)).ravel()\n",
    "pop_order = np.argsort(-item_pop)\n",
    "\n",
    "def rec_pop(u, k=K):\n",
    "    known = set(X[u].indices)\n",
    "    out = [i for i in pop_order if i not in known][:k]\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "# 2) Item-KNN (cosine on item co-occur)\n",
    "X_bin = X.copy()\n",
    "X_bin.data = np.ones_like(X_bin.data)  # co-occurrence only\n",
    "\n",
    "# items x users\n",
    "M = X_bin.T\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "M_norm = normalize(M, axis=1)  # L2 normalize each item vector (cosine)\n",
    "\n",
    "def rec_itemknn(u, k=10):\n",
    "    known = X[u].indices\n",
    "    if len(known) == 0:\n",
    "        return rec_pop(u, k)\n",
    "\n",
    "    # similarity of ALL items to each known item\n",
    "    sims = M_norm @ M_norm[known].T      # (n_items x |known|)\n",
    "    scores = np.asarray(sims.sum(axis=1)).ravel()\n",
    "\n",
    "    # don’t recommend what the user already has\n",
    "    scores[known] = -np.inf\n",
    "\n",
    "    if scores.size == 0: return []\n",
    "    k_eff = min(k, max(1, scores.size - 1))\n",
    "    top = np.argpartition(-scores, kth=k_eff-1)[:k_eff]\n",
    "    return top[np.argsort(-scores[top])][:k]\n",
    "\n",
    "\n",
    "# 3) TruncatedSVD (MF-ish)\n",
    "svd = TruncatedSVD(n_components=64, random_state=42)\n",
    "U = svd.fit_transform(X)     # user factors (n_users x d)\n",
    "V = svd.components_.T        # item factors (n_items x d)\n",
    "\n",
    "def rec_svd(u, k=10):\n",
    "    scores = U[u] @ V.T\n",
    "    scores[X[u].indices] = -np.inf  # mask knowns\n",
    "    if scores.size == 0: return []\n",
    "    k_eff = min(k, max(1, scores.size - 1))\n",
    "    top = np.argpartition(-scores, kth=k_eff-1)[:k_eff]\n",
    "    return top[np.argsort(-scores[top])][:k]\n",
    "\n",
    "\n",
    "\n",
    "# ==== METRICS AND EVALUATE ====\n",
    "def hit_rate_at_k(recs, truth): return 1.0 if any(t in recs for t in truth) else 0.0\n",
    "def recall_at_k(recs, truth):   return len(set(recs) & set(truth)) / len(truth)\n",
    "def ndcg_at_k(recs, truth):\n",
    "    dcg = 0.0\n",
    "    for r, i in enumerate(recs, start=1):\n",
    "        if i in truth: dcg += 1.0 / np.log2(r + 1)\n",
    "    idcg = sum(1.0 / np.log2(r + 1) for r in range(1, min(len(truth), len(recs)) + 1))\n",
    "    return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "def evaluate(recommender, name, K=10):\n",
    "    HR, REC, NDCG = [], [], []\n",
    "    for u, truth in test_truth.items():\n",
    "        recs = recommender(u, K)\n",
    "        HR.append(hit_rate_at_k(recs, truth))\n",
    "        REC.append(recall_at_k(recs, truth))\n",
    "        NDCG.append(ndcg_at_k(recs, truth))\n",
    "    print(f\"{name:12s} | HR@{K}: {np.mean(HR):.4f}  Recall@{K}: {np.mean(REC):.4f}  NDCG@{K}: {np.mean(NDCG):.4f}\")\n",
    "\n",
    "#RUN BASELINES\n",
    "#HR@10 is fraction of users whose hidden last item showed up in their top 10\n",
    "#Recall@10 is the number of relevant items in the top 10\n",
    "#NDCG means you not only include it in the top 10, but you also include it higher up in the top 10\n",
    "K = 10\n",
    "print(f\"Users: {n_users}, Items: {n_items}, Train events: {X.nnz}, Test users: {len(test_truth)}\")\n",
    "evaluate(rec_pop,     \"Popularity\", K)\n",
    "evaluate(rec_itemknn, \"Item-KNN\",   K)\n",
    "evaluate(rec_svd,     \"TruncSVD\",   K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 83636,
     "status": "ok",
     "timestamp": 1758730735716,
     "user": {
      "displayName": "Liu Zhengyang",
      "userId": "06130671106158642277"
     },
     "user_tz": -480
    },
    "id": "pywRw8s-g9fz",
    "outputId": "8a1a0c84-324d-40ee-8b97-547fb8be67ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# Make Surprise happy with NumPy < 2\n",
    "!pip -q install \"numpy<2\" cython\n",
    "!pip -q install scikit-surprise==1.1.3\n",
    "#OK NOW RESTART RUNTIME LOL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0qe9mjk9uhW"
   },
   "source": [
    "NEXT PART: NON-BASELINE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 92191,
     "status": "ok",
     "timestamp": 1758730931430,
     "user": {
      "displayName": "Liu Zhengyang",
      "userId": "06130671106158642277"
     },
     "user_tz": -480
    },
    "id": "Ki_tmrTD9xJT",
    "outputId": "1e116300-9ba2-4836-cd1a-0e686805bd22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD++        | HR@10: 0.0044  Recall@10: 0.0044  NDCG@10: 0.0018\n"
     ]
    }
   ],
   "source": [
    "#SVD\n",
    "\n",
    "!pip -q install scikit-surprise\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVDpp\n",
    "\n",
    "# Build a Surprise trainset from TRAIN interactions (implicit => rating=1.0)\n",
    "svdpp_train = train[[\"u\",\"i\"]].drop_duplicates().copy()\n",
    "svdpp_train[\"rating\"] = 1.0\n",
    "\n",
    "reader = Reader(rating_scale=(0, 1))\n",
    "data = Dataset.load_from_df(svdpp_train[[\"u\",\"i\",\"rating\"]], reader)\n",
    "trainset = data.build_full_trainset()\n",
    "\n",
    "algo = SVDpp(n_factors=64, n_epochs=20, random_state=42)\n",
    "algo.fit(trainset)\n",
    "\n",
    "def rec_svdpp(u, k=10):\n",
    "    known = set(X[u].indices)\n",
    "    # candidates = all unseen items\n",
    "    cand = [i for i in range(n_items) if i not in known]\n",
    "    if not cand:\n",
    "        return []\n",
    "    # predict for unseen items\n",
    "    preds = np.array([algo.predict(uid=int(u), iid=int(i), clip=False).est for i in cand])\n",
    "    k_eff = min(k, len(cand))\n",
    "    top = np.argpartition(-preds, k_eff-1)[:k_eff]\n",
    "    return [cand[idx] for idx in top[np.argsort(-preds[top])]]\n",
    "\n",
    "evaluate(rec_svdpp, \"SVD++\", 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2texY0ohC7AP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2feL4ZOGyV4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
